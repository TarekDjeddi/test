{
  "education": [
    {
      "awards_achievements": null,
      "degree": "Diploma in Data Science",
      "department": "Data Science",
      "duration": "2022-2022 (Intensive mode 3 Months)",
      "school_name": "Concordia University"
    },
    {
      "awards_achievements": [
        "GPA: 3.68/4.3"
      ],
      "degree": "Master of Applied Science in Electrical and Computer Engineering",
      "department": "Electrical and Computer Engineering",
      "duration": "2016-2018",
      "school_name": "Concordia University"
    },
    {
      "awards_achievements": [
        "Merit: 1st in cohort",
        "GPA: 3.8/4"
      ],
      "degree": "Bachelor of Science in Electrical Engineering/Telecommunication",
      "department": "Electrical Engineering",
      "duration": "2012-2016",
      "school_name": "Ajman Univerity, United Arab Emirates"
    }
  ],
  "github_repo_root_url": "https://github.com/jarviscanada/jarvis_data_eng_TarekDjeddi",
  "highlighted_projects": [
    {
      "description": "Stock Market Forecasting using Recurrent neural network (RNN) and AutoRegressive Integrated Moving Average (ARIMA).",
      "git_url": "https://github.com/TarekDjeddi/final_project/tree/main",
      "name": "Deep Learning"
    },
    {
      "description": "Image Classication using Convolutional Neural Network (CNN) on 400-bird species.",
      "git_url": "https://github.com/TarekDjeddi/ds-predictive-modelling-project-4",
      "name": "Predictive Modelling"
    },
    {
      "description": "Natural Language Processing (NLP), Dimensionality Reduction, and Clustering on COVID-19 tweets (API).",
      "git_url": "https://github.com/TarekDjeddi/ds-project-data-engineering-3",
      "name": "Unsupervised Machine Learning"
    },
    {
      "description": "Regression discontinuity design (RDD) to identify the eect of COVID-19 In Quebec.",
      "git_url": "https://github.com/TarekDjeddi/ds-regression-project-2",
      "name": "Regression"
    },
    {
      "description": "The Big Heist using Nearest Neighbor Algorithm. The bank heist algorithm is separated into several functions which used together to return a list of bank ID's.",
      "git_url": "https://github.com/TarekDjeddi/ds-algorithm-project-1",
      "name": "Algorithms"
    }
  ],
  "jarvis_projects": [
    {
      "description": "Developed a Linux Cluster Monitoring Agent which helps Jarvis Linux Cluster Administration (LCA) team to track hardware information and recourse usage of each node in real-time, which are running on CentOS. The resource usage data were collected and saved in a PostgreSQL database automatically every minute in the background, while the hardware specifications were assumed to be static, which means that they will be collected once only. Technolgies that been used to successfully develop the project are IntelliJ IDEA, Bash, Docker, PostgreSQL, SQL, Git, and GitHub.",
      "git_url": "/linux_sql",
      "name": "Cluster Monitor"
    },
    {
      "description": "Developed a Linux Cluster Monitoring Agent which helps Jarvis Linux Cluster Administration (LCA) team to track hardware information and recourse usage of each node in real-time, which are running on CentOS 7. The resource usage data were collected and saved in a PostgreSQL database automatically every minute in the background, while the hardware specifications were assumed to be static, which means that they will be collected once only. Technologies that have been used to successfully develop the project are IntelliJ IDEA, Bash, Docker, PostgreSQL, SQL, Git, and GitHub.",
      "git_url": "/python_data_anlytics",
      "name": "Python Data Analytics"
    },
    {
      "description": "Apache Hadoop is used in this project to process and analyze the World Development Indicators (WDI) dataset, which contains around 21 million data points. We evaluated Core Hadoop components, including MapReduce, HDFS, and YARN. We created three nodes Hadoop clusters (one master and two worker nodes), which are provisioned by the Google Cloud Platform (GCP) DataProc service. Apache Hive and Zeppelin Notebook were used to solve some business problems and analyze the data using HiveQL queries. Optimization techniques like Hive Partition and Columnar File Optimization have been used to speed up the execution time of queries.",
      "git_url": "/hadoop",
      "name": "Hadoop"
    }
  ],
  "name": "Tarek Djeddi",
  "others": [
    {
      "bullets": [
        "Football",
        "Mobile Gamming"
      ],
      "title": "Activities/Hobbies"
    }
  ],
  "professional_experience": [
    {
      "company": "Jarvis",
      "description": "Working with a team in an Agile environment. Developed a Linux Cluster Monitoring Agent to help the Linux Cluster Administrator team monitor the hardware specifications and resource usage. Visualized and analyzed London Gift Shop data to help them make decisions to increase their revenue. Used Hadoop to analyze big data, optimize query execution time, and gain an in-depth understanding of Hadoop key components such as HDFS, MapReduce, and YARN.",
      "duration": "2022-present",
      "title": "Data Engineer/Data Analyst"
    },
    {
      "company": "Amike Fire Alarm Security Service",
      "description": "Assessing the performance of different systems, and writing inspection and verification reports. Communicating with clients to understand their problems and solve them.",
      "duration": "2020-2021",
      "title": "Fire Alarm Technician"
    },
    {
      "company": "Services VortexApp Inc.",
      "description": "Creating and designing Websites for clients, in English and French. Creating several one-page and multi-page templates. Set up VortexApp phones by enabling the necessary applications for the encrypted phones, and activating SIM Cards.",
      "duration": "2019-2019",
      "title": "Web Desinger"
    }
  ],
  "skills": {
    "competent": [
      "Linux/Bash",
      "RDBM",
      "Predictive Modeling",
      "Hadoop",
      "Zeppelin Notebook"
    ],
    "familiar": [
      "HTML",
      "CSS",
      "C++",
      "Java",
      "MATLAB",
      "Google Cloud Platform"
    ],
    "proficient": [
      "Python",
      "Juypter Notebook",
      "Machine Learning",
      "Data Analysis",
      "SQL/HiveQL",
      "Agile/Scrum",
      "Git"
    ]
  },
  "summary": "I completed a Diploma in Data Science at Concordia University which I took due to my strong interest in data analysis and modeling. I am a holder of a Master of Applied Science in Electrical and Computer Engineering whose thesis focused on modeling and simulating Digitally Controlled Microwave Components for High-Power RADAR Systems. I aspire to build a successful career as a data engineer or as a data analyst."
}
